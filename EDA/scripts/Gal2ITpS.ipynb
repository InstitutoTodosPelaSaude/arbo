{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Gal data into combined format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libs \n",
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read files combined_arbo.xlsx and gal_am_12_12_23.txt \n",
    "### ITpS\n",
    "combined_arbo = pd.read_excel('combined_arbo.xlsx')\n",
    "### GAL\n",
    "gal_am_12_12_23 = pd.read_csv('gal_am_12_12_23', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check structure of files\n",
    "### ITpS\n",
    "print(combined_arbo.head())\n",
    "## list of columns\n",
    "print(combined_arbo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check structure of files\n",
    "### GAL\n",
    "print(gal_am_12_12_23.head())\n",
    "## list of columns\n",
    "print(gal_am_12_12_23.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new DataFrame with the same columns as combined_arbo\n",
    "## Initially, all columns are empty\n",
    "combined_structure_columns = [\n",
    "    'lab_id', 'sample_id', 'test_id', 'test_kit', 'gender', 'age',\n",
    "    'location', 'date_testing', 'state', 'patient_id', 'file_name',\n",
    "    'denv_test_result', 'zikv_test_result', 'chikv_test_result',\n",
    "    'yfv_test_result', 'mayv_test_result', 'orov_test_result',\n",
    "    'wnv_test_result', 'qty_original_lines', 'created_at', 'updated_at',\n",
    "    'age_group', 'epiweek', 'month', 'country', 'region', 'macroregion',\n",
    "    'macroregion_code', 'state_code', 'state_ibge_code',\n",
    "    'location_ibge_code', 'lat', 'long'\n",
    "]\n",
    "new_df = pd.DataFrame(columns=combined_structure_columns)\n",
    "\n",
    "## Populate columns according to instructions\n",
    "new_df['lab_id'] = 'CGLAB'  # Fixed value\n",
    "new_df['test_id'] = gal_am_12_12_23['cod_amostra']\n",
    "new_df['test_kit'] = gal_am_12_12_23['exame'].replace({'Dengue, Detecção de Antígeno NS1': 'NS1_antigen'})\n",
    "new_df['location'] = gal_am_12_12_23['mun_residencia']\n",
    "new_df['state_code'] = gal_am_12_12_23['uf_residencia']\n",
    "new_df['date_testing'] = pd.to_datetime(gal_am_12_12_23['dt_cadastro'], format='%d/%m/%Y %H:%M:%S')\n",
    "new_df['denv_test_result'] = gal_am_12_12_23['resultado'].replace({'Reagente': 'Pos', 'Não Reagente': 'Neg'})\n",
    "\n",
    "## Generate hash for sample_id based on specific columns of gal_am_12_12_23\n",
    "def generate_hash(row):\n",
    "    hash_input = str(row['requisicao']) + str(row['cod_amostra'])\n",
    "    return hashlib.sha256(hash_input.encode()).hexdigest()\n",
    "\n",
    "## Applying the hash generation function for the sample_id column\n",
    "new_df['sample_id'] = gal_am_12_12_23.apply(generate_hash, axis=1)\n",
    "\n",
    "## Save the transformed DataFrame to a TSV file\n",
    "tsv_file_path = 'combined_arbo_gal.tsv'\n",
    "new_df.to_csv(tsv_file_path, sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
